version: '3.8'

services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    ports:
      - '4000:4000'
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - LITELLM_LOG_LEVEL=INFO
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      # Proxy settings for LiteLLM to access OpenRouter API
      # These will use values from .env file if set, otherwise use defaults
      - http_proxy=${HTTP_PROXY:-}
      - https_proxy=${HTTPS_PROXY:-}
      - HTTP_PROXY=${HTTP_PROXY:-}
      - HTTPS_PROXY=${HTTPS_PROXY:-}
      - no_proxy=${NO_PROXY:-localhost,127.0.0.1,::1}
      - NO_PROXY=${NO_PROXY:-localhost,127.0.0.1,::1}      
    command:
      ['--config', '/app/config.yaml', '--port', '4000', '--num_workers', '1']
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'sh', '-c', 'python -c "import urllib.request,os; urllib.request.urlopen(urllib.request.Request(\"http://localhost:4000/health\", headers={\"Authorization\": \"Bearer \" + os.environ.get(\"LITELLM_MASTER_KEY\", \"\")}))"']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - litellm-network

networks:
  litellm-network:
    driver: bridge
